{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDN Model training demo\n",
    "\n",
    "This notebook demonstrates the training process of the MDN model using a small subset of our main training data set.\n",
    "\n",
    "All necessary data used in this notebook can be found in the ./data/training_demo directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:07:42.265949Z",
     "start_time": "2023-02-23T10:07:42.245058Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "%load_ext tensorboard\n",
    "\n",
    "from tensorflow_probability import distributions as tfd\n",
    "\n",
    "from tensorflow.keras.layers import Input, InputLayer, Dense, Activation, Concatenate, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import exomdn.mdn_layer as mdn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from scipy.stats import norm as normal\n",
    "import joblib\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:19.424711Z",
     "start_time": "2023-02-22T16:42:19.418796Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"TF version: {tf.__version__}\")\n",
    "print(\"Available devices:\")\n",
    "pprint(tf.config.list_physical_devices(), width=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the MDN architecture\n",
    "\n",
    "We are using a custom MDN layer from https://github.com/cpmpercussion/keras-mdn-layer\n",
    "\n",
    "The training architecture and parameters are defined in the model_parameters.json file and used in the `build_mdn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-22T16:42:19.437650Z",
     "start_time": "2023-02-22T16:42:19.427740Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_mdn(n_inputs=2, n_outputs=1, n_layers=2, units_per_layer=100, components=10, **kwargs):\n",
    "    learning_rate = kwargs.get(\"learning_rate\", 0.001)\n",
    "\n",
    "    model = tf.keras.Sequential(name=\"MDN\")\n",
    "    # Input layer\n",
    "    model.add(InputLayer(input_shape=(n_inputs,), name=\"input\"))\n",
    "    for i in range(n_layers):\n",
    "        # Hidden layers\n",
    "        model.add(Dense(units_per_layer, activation=\"relu\", kernel_initializer=\"glorot_uniform\", name=f\"relu_{i}\"))\n",
    "    \n",
    "    # Add MDN output layer\n",
    "    model.add(mdn.MDN(n_outputs, components, name=\"output_mdn\"))\n",
    "    \n",
    "    # use custom loss (negative loss likelihood for training) \n",
    "    model.compile(loss=mdn.get_mixture_loss_func(n_outputs, components),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T09:51:43.556665Z",
     "start_time": "2023-02-23T09:51:43.545374Z"
    }
   },
   "outputs": [],
   "source": [
    "config_path = Path(\"../data/training_demo/\")\n",
    "data_path = Path(\"../data/training_demo/\")\n",
    "models_path = Path(\"../models\")\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Tensorboard log directory\n",
    "tf_logs = (models_path / \"_tf_logs\")\n",
    "tf_logs.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:12:34.997070Z",
     "start_time": "2023-02-23T10:12:34.990479Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(config_path / \"model_parameters.json\", \"r\") as f:\n",
    "    parameters = json.load(f)\n",
    "    \n",
    "# parameter_space.json contains min/max values for input parameters, for checking parameter ranges during predictions\n",
    "with open(config_path / \"parameter_space.json\", \"r\") as f:\n",
    "    parameter_space = json.load(f)\n",
    "\n",
    "inputs = parameters[\"inputs\"]\n",
    "outputs = parameters[\"outputs\"]\n",
    "model_id = parameters[\"model_id\"]\n",
    "seed = parameters.get(\"seed\", 42)\n",
    "save_path = models_path / model_id\n",
    "log_path = tf_logs / model_id\n",
    "\n",
    "# write input parameter ranges to config file\n",
    "parameters[\"input_properties\"]= {key: parameter_space[key] for key in inputs if key in parameter_space}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:12:35.573132Z",
     "start_time": "2023-02-23T10:12:35.468475Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Loading data: \" + str(data_path / parameters[\"training_data\"]))\n",
    "columns = inputs + outputs\n",
    "data = pd.read_csv(data_path / parameters[\"training_data\"], usecols=columns)[columns]\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:12:37.672323Z",
     "start_time": "2023-02-23T10:12:37.647566Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = data[inputs]\n",
    "Y = data[outputs]\n",
    "\n",
    "# log transform planet_mass column\n",
    "mass_transf = ColumnTransformer([(\"log_mass\", FunctionTransformer(np.log10), [X.columns.get_loc(\"planet_mass\")])], remainder=\"passthrough\")\n",
    "preprocessor = Pipeline([(\"log_mass\", mass_transf),\n",
    "                         (\"scaler\", StandardScaler())])\n",
    "X_scaled = preprocessor.fit_transform(X)\n",
    "Y_scaled = np.array(Y)\n",
    "\n",
    "# split into train/test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y_scaled, test_size=parameters[\"validation_size\"],\n",
    "                                                    random_state=seed, shuffle=True)\n",
    "print(f\"X_train: {X_train.shape} | Y_train: {Y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape} | Y_test: {Y_test.shape}\")\n",
    "print(f\"Inputs: {inputs}\\nPredict: {outputs}\")\n",
    "\n",
    "# Save preprocessor and parameters\n",
    "print(\"Saving preprocessor and parameters...\")\n",
    "joblib.dump(preprocessor, save_path / \"preprocessor.pkl\")\n",
    "with open(save_path / \"setup_parameters.json\", \"w\") as fp:\n",
    "    json.dump(parameters, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDN setup\n",
    "\n",
    "We are using TensorBoard to track training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:12:38.661141Z",
     "start_time": "2023-02-23T10:12:38.549689Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tensorboard_mdn = TensorBoard(log_dir=log_path, histogram_freq=5, write_graph=False, write_images=False)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=parameters[\"patience\"], verbose=1, mode=\"auto\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=parameters.get(\"patience_lr\", 4), verbose=1, min_delta=0)\n",
    "\n",
    "print(f\"Log directory: {log_path}\")\n",
    "\n",
    "mdn_model = build_mdn(n_inputs=len(inputs), n_outputs=len(outputs), **parameters[\"architecture\"])\n",
    "mdn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:20:43.171971Z",
     "start_time": "2023-02-23T10:20:43.158195Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Start TensorBoard monitor to monitor training.\n",
    "%tensorboard --logdir {tf_logs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T10:14:16.438329Z",
     "start_time": "2023-02-23T10:12:45.507423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "mdn_model.fit(x=X_train, y=Y_train, validation_data=(X_test, Y_test),\n",
    "              epochs=parameters[\"epochs\"], batch_size=parameters[\"batch_size\"],\n",
    "              callbacks=[reduce_lr, early_stopping, tensorboard_mdn], verbose=1)\n",
    "\n",
    "# Save model\n",
    "tf.keras.models.save_model(mdn_model, save_path / \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
